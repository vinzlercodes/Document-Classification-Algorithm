{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Document Classification using Hierarchical Attention Networks**","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-09T23:19:14.914091Z","iopub.execute_input":"2021-12-09T23:19:14.914819Z","iopub.status.idle":"2021-12-09T23:19:20.566640Z","shell.execute_reply.started":"2021-12-09T23:19:14.914703Z","shell.execute_reply":"2021-12-09T23:19:20.565868Z"}}},{"cell_type":"markdown","source":"# Importing Libraries","metadata":{}},{"cell_type":"code","source":"from keras.preprocessing import sequence\nfrom keras.models import Model\nfrom keras.layers import Dense, Embedding,Input,Bidirectional,TimeDistributed,Activation,Lambda,Multiply,Dropout\nfrom keras.layers import LSTM,GRU\nimport keras.backend as K\nfrom tensorflow.keras.optimizers import Adam\nimport numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport re\nfrom keras.preprocessing.text import Tokenizer, text_to_word_sequence\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils.np_utils import to_categorical\nfrom keras.utils.vis_utils import plot_model\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# IMDB Datset","metadata":{}},{"cell_type":"code","source":"data_path ='../input/nlp-project/labeledTrainData.tsv'\ndata_train = pd.read_csv(data_path, sep='\\t')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"# Shape Confirmation\ndata_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Pre-processing","metadata":{}},{"cell_type":"markdown","source":"# Cleaning","metadata":{}},{"cell_type":"code","source":"def clean_str(string):\n    \"\"\"\n    Tokenization/string cleaning for dataset\n    Every dataset is lower cased except\n    \"\"\"\n    string = re.sub(r\"\\\\\", \"\", string)    \n    string = re.sub(r\"\\'\", \"\", string)    \n    string = re.sub(r\"\\\"\", \"\", string)    \n    return string.strip().lower()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenization","metadata":{}},{"cell_type":"code","source":"from nltk import tokenize\n\nreviews = []\nlabels = []\ntexts = []\n\nfor idx in range(data_train.review.shape[0]):\n    text = BeautifulSoup(data_train.review[idx],'lxml')\n    text = clean_str(text.get_text())\n    texts.append(text)\n    sentences = tokenize.sent_tokenize(text)\n    reviews.append(sentences)\n    \n    labels.append(data_train.sentiment[idx])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualizing the pre-processed data","metadata":{}},{"cell_type":"code","source":"reviews[45]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_SENT_LENGTH = 100  # Maximum number of words per sentence\nMAX_SENTS = 15    # Maximum number of sentences per document\nMAX_NB_WORDS = 20000  # Maximum number of words to use\nVALIDATION_SPLIT = 0.2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\ntokenizer.fit_on_texts(texts)\n\ndata = np.zeros((len(texts), MAX_SENTS, MAX_SENT_LENGTH), dtype='int32')\n\nfor i, sentences in enumerate(reviews):\n    for j, sent in enumerate(sentences):\n        if j< MAX_SENTS:\n            wordTokens = text_to_word_sequence(sent)\n            k=0\n            for _, word in enumerate(wordTokens):\n                if k<MAX_SENT_LENGTH and tokenizer.word_index[word]<MAX_NB_WORDS:\n                    data[i,j,k] = tokenizer.word_index[word]\n                    k=k+1                    \n                    \nword_index = tokenizer.word_index\nprint('Total %s unique tokens.' % len(word_index))\n\nlabels = to_categorical(np.asarray(labels))\nprint('Shape of data:', data.shape)\nprint('Shape of label tensor:', labels.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training and Validation Sets","metadata":{}},{"cell_type":"code","source":"indices = np.arange(data.shape[0])\nnp.random.shuffle(indices)\ndata = data[indices]\nlabels = labels[indices]\nnb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n\nx_train = data[:-nb_validation_samples]\ny_train = labels[:-nb_validation_samples]\nx_val = data[-nb_validation_samples:]\ny_val = labels[-nb_validation_samples:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('train and validation shapes in data')\nprint(y_train.sum(axis=0))\nprint(y_val.sum(axis=0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Defining the Sentence and Word Embeddings","metadata":{}},{"cell_type":"markdown","source":"# Building a model","metadata":{}},{"cell_type":"code","source":"#---------Word Embedding-----------\n\nhid_dim = 64  # Number of dimensions of hidden layer\nembed_size = 128 # Number of embedded dimensions of a word\natt_dim = 32  # attention dimension of output in fully connected layer for calculation of\ndrop =0.5 # Dropout\nout_size=2 # Final output number\n\n#---------Sentence Embedding-----------\n\nhid_dim2 = 64  # Number of dimensions of hidden layer\natt_dim2 = 32  # attention dimension of output in fully connected layer for calculation of","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Defining the Model Architecture","metadata":{}},{"cell_type":"code","source":"inputs =Input(shape=(MAX_SENT_LENGTH,))\nemb= Embedding(MAX_NB_WORDS , embed_size)(inputs)  #Embedding layer\n\nstate = Bidirectional(GRU(hid_dim, return_sequences=True))(emb)  # GRU Model\n\n#-------  Word Attention  ----\n\nu_it = TimeDistributed(Dense(att_dim, activation='tanh'),name='T1')(state)\nscore = TimeDistributed(Dense(1),name='T2')(u_it)\nscore_ = Lambda(lambda x: K.reshape(x, (K.shape(x)[0], MAX_SENT_LENGTH)))(score)\nalpha=Activation('softmax')(score_)\nalpha_ = Lambda(lambda x: K.expand_dims(x))(alpha)\nalphahs=Multiply(name='attention_mul')([alpha_,state])\ns = Lambda(lambda x: K.sum(x, axis=1))(alphahs)\n\nsent_Encoder = Model(inputs, s)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Defining Model Parameters ","metadata":{}},{"cell_type":"code","source":"doc_input = Input(shape=(MAX_SENTS,MAX_SENT_LENGTH))\nencoded_sent = TimeDistributed(sent_Encoder)(doc_input)\n\nstate2= Bidirectional(GRU(hid_dim2, return_sequences=True))(encoded_sent)\n\n#-------  Sentence Attention  ----\n\nu_it2 = TimeDistributed(Dense(att_dim2, activation='tanh'),name='T1')(state2)\nscore2 = TimeDistributed(Dense(1),name='T2')(u_it2)\nscore_2 = Lambda(lambda x: K.reshape(x, (K.shape(x)[0], MAX_SENTS)))(score2)\nalpha2=Activation('softmax')(score_2)\nalpha_2 = Lambda(lambda x: K.expand_dims(x))(alpha2)\n\nalphahs2=Multiply(name='attention_mul')([alpha_2,state2])\ns2 = Lambda(lambda x: K.sum(x, axis=1))(alphahs2)\n\npreds = Dense(out_size, activation='softmax')(s2)\nmodel = Model(doc_input, preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualising the Model","metadata":{}},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyper-parameters for the Model Training","metadata":{}},{"cell_type":"code","source":"adam = Adam(learning_rate=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n\nmodel.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 2\nbatch_size = 32","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, shuffle=True, validation_data=(x_val,y_val)) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Attention Visualization","metadata":{}},{"cell_type":"code","source":"# Converting the ID column of the data\n\nid_to_word = {value:key for key,value in tokenizer.word_index.items()}\nid_to_word[0]=''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num=3\n\nprint('Meta data of the article')\nL=[]\nfor i,sen in enumerate(x_val[num]):\n    if np.sum(sen)>0:\n        l = ' '.join([id_to_word[id]for  id in sen])\n        L.append(l)\nprint(L)\nprint('Number of articles:',len(L))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Attention Model Output","metadata":{}},{"cell_type":"code","source":"# attention weight\nget_Attention = K.function([model.layers[0].input],\n                                  [model.layers[6].output])\nattention = get_Attention([x_val[num:num+1]])[0][0]\nprint(attention.shape)\nprint('attention weight Maximum Index Articles'.format(np.argmax(attention)+1,))\n\nplt.plot(attention)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# attentionã€€weight the total of 1 \nnp.sum(attention)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# To each sentence attention weight\nxtick = ['sentence{}'.format(i) for i,_ in enumerate(L)]\nplt.figure(figsize=(5, 1))\nprint(xtick)\nplt.pcolormesh(attention[np.newaxis,:]\n#sns.heatmap(attention[np.newaxis,:],cmap=\"Blues\",xticklabels=xtick,yticklabels=False)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}